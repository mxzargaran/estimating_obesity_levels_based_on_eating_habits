{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== OBESITY LEVEL PREDICTION MODEL =====\n",
      "This model predicts obesity levels based on demographic and health-related factors.\n",
      "Research Question: What demographic and health-related factors significantly predict obesity levels in individuals?\n",
      "============================================================\n",
      "\n",
      "===== DATA LOADING AND EXPLORATION =====\n",
      "Loading the dataset and exploring its basic characteristics.\n",
      "This helps us understand the data structure and identify potential issues.\n",
      "\n",
      "===== DATASET OVERVIEW =====\n",
      "Dataset shape: (2087, 17)\n",
      "Number of samples: 2087\n",
      "Number of features: 16\n",
      "Column names: Gender, Age, Height, Weight, family_history_with_overweight, FAVC, FCVC, NCP, CAEC, SMOKE, CH2O, SCC, FAF, TUE, CALC, MTRANS, NObeyesdad\n",
      "Target variable: NObeyesdad\n",
      "\n",
      "Missing values per column:\n",
      "No missing values found\n",
      "\n",
      "Checking for outliers in numerical columns:\n",
      "Age: 167 outliers detected\n",
      "Height: 1 outliers detected\n",
      "Weight: 1 outliers detected\n",
      "FCVC: 0 outliers detected\n",
      "NCP: 577 outliers detected\n",
      "CH2O: 0 outliers detected\n",
      "FAF: 0 outliers detected\n",
      "TUE: 0 outliers detected\n",
      "\n",
      "===== FEATURE TYPES =====\n",
      "Identified 8 categorical columns: ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
      "Identified 8 numerical columns: ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
      "============================================================\n",
      "\n",
      "===== DATA PREPROCESSING =====\n",
      "Preparing the data for model training by encoding categorical variables,\n",
      "scaling numerical features, and splitting into training and testing sets.\n",
      "This step ensures our data is in the correct format for machine learning algorithms.\n",
      "Classes mapping: {'Insufficient_Weight': 0, 'Normal_Weight': 1, 'Obesity_Type_I': 2, 'Obesity_Type_II': 3, 'Obesity_Type_III': 4, 'Overweight_Level_I': 5, 'Overweight_Level_II': 6}\n",
      "\n",
      "Target Variable Distribution:\n",
      "NObeyesdad\n",
      "Obesity_Type_I         351\n",
      "Obesity_Type_III       324\n",
      "Obesity_Type_II        297\n",
      "Overweight_Level_II    290\n",
      "Normal_Weight          282\n",
      "Overweight_Level_I     276\n",
      "Insufficient_Weight    267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Encoded Target Variable Distribution:\n",
      "2    351\n",
      "4    324\n",
      "3    297\n",
      "6    290\n",
      "1    282\n",
      "5    276\n",
      "0    267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class imbalance ratio (max/min): 1.31\n",
      "Dataset shape after one-hot encoding: (2087, 23)\n",
      "Training set: (1669, 23), Test set: (418, 23)\n",
      "Scaler saved to '../models/scaler.pkl'\n",
      "Label encoder saved to '../models/label_encoder.pkl'\n",
      "Feature column names saved to '../models/feature_columns.pkl'\n",
      "============================================================\n",
      "\n",
      "===== FEATURE SELECTION =====\n",
      "Identifying the most important features for predicting obesity levels.\n",
      "This step helps us focus on the most relevant factors and build more efficient models.\n",
      "We'll use a Random Forest classifier to rank features by importance.\n",
      "\n",
      "Top 15 features by importance:\n",
      "Weight: 0.2952\n",
      "Age: 0.0982\n",
      "Height: 0.0912\n",
      "FCVC: 0.0889\n",
      "Gender_Male: 0.0600\n",
      "NCP: 0.0555\n",
      "FAF: 0.0496\n",
      "TUE: 0.0469\n",
      "CH2O: 0.0464\n",
      "family_history_with_overweight_yes: 0.0312\n",
      "CALC_Sometimes: 0.0233\n",
      "CAEC_Sometimes: 0.0214\n",
      "MTRANS_Public_Transportation: 0.0200\n",
      "FAVC_yes: 0.0181\n",
      "CALC_no: 0.0170\n",
      "\n",
      "Number of features selected: 9 out of 23\n",
      "Selected features: ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'Gender_Male']\n",
      "\n",
      "Feature matrix shapes after selection: Train (1669, 9), Test (418, 9)\n",
      "Feature selector saved to '../models/feature_selector.pkl'\n",
      "============================================================\n",
      "\n",
      "===== MODEL TRAINING AND EVALUATION =====\n",
      "Training multiple machine learning models and evaluating their performance.\n",
      "We'll compare different algorithms to find the best approach for predicting obesity levels.\n",
      "Each model will be evaluated using cross-validation and tested on the holdout test set.\n",
      "\n",
      "=== Model Evaluation with All Features ===\n",
      "\n",
      "Training Logistic Regression with all features...\n",
      "CV scores: [0.85329341 0.91916168 0.83532934 0.88323353 0.86186186]\n",
      "Mean CV score: 0.8706, Std: 0.0288\n",
      "Test accuracy: 0.8947\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        53\n",
      "           1       0.84      0.81      0.82        57\n",
      "           2       0.92      0.93      0.92        70\n",
      "           3       0.94      0.98      0.96        60\n",
      "           4       0.98      0.98      0.98        65\n",
      "           5       0.80      0.80      0.80        55\n",
      "           6       0.83      0.74      0.78        58\n",
      "\n",
      "    accuracy                           0.89       418\n",
      "   macro avg       0.89      0.89      0.89       418\n",
      "weighted avg       0.89      0.89      0.89       418\n",
      "\n",
      "\n",
      "Training Logistic Regression (Balanced) with all features...\n",
      "CV scores: [0.85329341 0.91616766 0.84730539 0.88023952 0.85885886]\n",
      "Mean CV score: 0.8712, Std: 0.0251\n",
      "Test accuracy: 0.8995\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        53\n",
      "           1       0.84      0.81      0.82        57\n",
      "           2       0.93      0.93      0.93        70\n",
      "           3       0.94      0.98      0.96        60\n",
      "           4       0.98      0.98      0.98        65\n",
      "           5       0.80      0.82      0.81        55\n",
      "           6       0.86      0.76      0.81        58\n",
      "\n",
      "    accuracy                           0.90       418\n",
      "   macro avg       0.90      0.90      0.90       418\n",
      "weighted avg       0.90      0.90      0.90       418\n",
      "\n",
      "\n",
      "Training Decision Tree with all features...\n",
      "CV scores: [0.91616766 0.93113772 0.92814371 0.91317365 0.90690691]\n",
      "Mean CV score: 0.9191, Std: 0.0092\n",
      "Test accuracy: 0.9282\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97        53\n",
      "           1       0.83      0.91      0.87        57\n",
      "           2       0.93      0.91      0.92        70\n",
      "           3       0.97      0.97      0.97        60\n",
      "           4       1.00      0.98      0.99        65\n",
      "           5       0.90      0.78      0.83        55\n",
      "           6       0.90      0.97      0.93        58\n",
      "\n",
      "    accuracy                           0.93       418\n",
      "   macro avg       0.93      0.93      0.93       418\n",
      "weighted avg       0.93      0.93      0.93       418\n",
      "\n",
      "\n",
      "Training Random Forest with all features...\n",
      "CV scores: [0.9251497  0.9491018  0.93413174 0.94311377 0.94594595]\n",
      "Mean CV score: 0.9395, Std: 0.0087\n",
      "Test accuracy: 0.9593\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        53\n",
      "           1       0.82      0.96      0.89        57\n",
      "           2       1.00      0.97      0.99        70\n",
      "           3       1.00      1.00      1.00        60\n",
      "           4       1.00      0.98      0.99        65\n",
      "           5       0.96      0.89      0.92        55\n",
      "           6       0.95      0.95      0.95        58\n",
      "\n",
      "    accuracy                           0.96       418\n",
      "   macro avg       0.96      0.96      0.96       418\n",
      "weighted avg       0.96      0.96      0.96       418\n",
      "\n",
      "\n",
      "Training KNN (k=5) with all features...\n",
      "CV scores: [0.7994012  0.80838323 0.82335329 0.82634731 0.83783784]\n",
      "Mean CV score: 0.8191, Std: 0.0136\n",
      "Test accuracy: 0.8397\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87        53\n",
      "           1       0.77      0.40      0.53        57\n",
      "           2       0.83      0.96      0.89        70\n",
      "           3       0.91      1.00      0.95        60\n",
      "           4       0.96      1.00      0.98        65\n",
      "           5       0.72      0.69      0.70        55\n",
      "           6       0.83      0.83      0.83        58\n",
      "\n",
      "    accuracy                           0.84       418\n",
      "   macro avg       0.83      0.83      0.82       418\n",
      "weighted avg       0.83      0.84      0.83       418\n",
      "\n",
      "\n",
      "Training KNN (k=7) with all features...\n",
      "CV scores: [0.77844311 0.7994012  0.7994012  0.7994012  0.81381381]\n",
      "Mean CV score: 0.7981, Std: 0.0113\n",
      "Test accuracy: 0.8110\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.98      0.85        53\n",
      "           1       0.82      0.32      0.46        57\n",
      "           2       0.80      0.90      0.85        70\n",
      "           3       0.88      1.00      0.94        60\n",
      "           4       0.94      1.00      0.97        65\n",
      "           5       0.71      0.65      0.68        55\n",
      "           6       0.75      0.78      0.76        58\n",
      "\n",
      "    accuracy                           0.81       418\n",
      "   macro avg       0.81      0.80      0.79       418\n",
      "weighted avg       0.81      0.81      0.79       418\n",
      "\n",
      "\n",
      "=== Model Evaluation with Selected Features ===\n",
      "\n",
      "Training Logistic Regression with selected features...\n",
      "CV scores: [0.85628743 0.9011976  0.86227545 0.86227545 0.83783784]\n",
      "Mean CV score: 0.8640, Std: 0.0207\n",
      "Test accuracy: 0.9019\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        53\n",
      "           1       0.92      0.77      0.84        57\n",
      "           2       0.97      0.99      0.98        70\n",
      "           3       0.97      1.00      0.98        60\n",
      "           4       0.98      0.98      0.98        65\n",
      "           5       0.73      0.73      0.73        55\n",
      "           6       0.81      0.81      0.81        58\n",
      "\n",
      "    accuracy                           0.90       418\n",
      "   macro avg       0.90      0.90      0.90       418\n",
      "weighted avg       0.90      0.90      0.90       418\n",
      "\n",
      "\n",
      "Training Logistic Regression (Balanced) with selected features...\n",
      "CV scores: [0.84730539 0.89520958 0.84730539 0.86526946 0.84684685]\n",
      "Mean CV score: 0.8604, Std: 0.0188\n",
      "Test accuracy: 0.8923\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        53\n",
      "           1       0.91      0.75      0.83        57\n",
      "           2       0.97      0.94      0.96        70\n",
      "           3       0.97      1.00      0.98        60\n",
      "           4       0.98      0.98      0.98        65\n",
      "           5       0.73      0.73      0.73        55\n",
      "           6       0.77      0.81      0.79        58\n",
      "\n",
      "    accuracy                           0.89       418\n",
      "   macro avg       0.89      0.89      0.89       418\n",
      "weighted avg       0.89      0.89      0.89       418\n",
      "\n",
      "\n",
      "Training Decision Tree with selected features...\n",
      "CV scores: [0.93113772 0.95808383 0.9251497  0.91616766 0.91291291]\n",
      "Mean CV score: 0.9287, Std: 0.0161\n",
      "Test accuracy: 0.9402\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        53\n",
      "           1       0.91      0.86      0.88        57\n",
      "           2       0.96      0.94      0.95        70\n",
      "           3       0.97      0.97      0.97        60\n",
      "           4       1.00      0.98      0.99        65\n",
      "           5       0.87      0.87      0.87        55\n",
      "           6       0.89      0.97      0.93        58\n",
      "\n",
      "    accuracy                           0.94       418\n",
      "   macro avg       0.94      0.94      0.94       418\n",
      "weighted avg       0.94      0.94      0.94       418\n",
      "\n",
      "\n",
      "Training Random Forest with selected features...\n",
      "CV scores: [0.9491018  0.96706587 0.94311377 0.94311377 0.96396396]\n",
      "Mean CV score: 0.9533, Std: 0.0103\n",
      "Test accuracy: 0.9617\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        53\n",
      "           1       0.93      0.91      0.92        57\n",
      "           2       1.00      0.97      0.99        70\n",
      "           3       0.98      1.00      0.99        60\n",
      "           4       1.00      0.98      0.99        65\n",
      "           5       0.89      0.91      0.90        55\n",
      "           6       0.93      0.97      0.95        58\n",
      "\n",
      "    accuracy                           0.96       418\n",
      "   macro avg       0.96      0.96      0.96       418\n",
      "weighted avg       0.96      0.96      0.96       418\n",
      "\n",
      "\n",
      "Training KNN (k=5) with selected features...\n",
      "CV scores: [0.78742515 0.79041916 0.79640719 0.7994012  0.7987988 ]\n",
      "Mean CV score: 0.7945, Std: 0.0048\n",
      "Test accuracy: 0.8014\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.83        53\n",
      "           1       0.70      0.40      0.51        57\n",
      "           2       0.78      0.87      0.82        70\n",
      "           3       0.92      1.00      0.96        60\n",
      "           4       0.98      1.00      0.99        65\n",
      "           5       0.69      0.62      0.65        55\n",
      "           6       0.70      0.72      0.71        58\n",
      "\n",
      "    accuracy                           0.80       418\n",
      "   macro avg       0.79      0.79      0.78       418\n",
      "weighted avg       0.79      0.80      0.79       418\n",
      "\n",
      "\n",
      "Training KNN (k=7) with selected features...\n",
      "CV scores: [0.76646707 0.7754491  0.78742515 0.7994012  0.78678679]\n",
      "Mean CV score: 0.7831, Std: 0.0113\n",
      "Test accuracy: 0.7847\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.91      0.81        53\n",
      "           1       0.65      0.39      0.48        57\n",
      "           2       0.77      0.86      0.81        70\n",
      "           3       0.87      1.00      0.93        60\n",
      "           4       0.98      1.00      0.99        65\n",
      "           5       0.69      0.60      0.64        55\n",
      "           6       0.70      0.69      0.70        58\n",
      "\n",
      "    accuracy                           0.78       418\n",
      "   macro avg       0.77      0.78      0.77       418\n",
      "weighted avg       0.78      0.78      0.77       418\n",
      "\n",
      "\n",
      "=== Model Performance Comparison ===\n",
      "                            Model  All Features  Selected Features  Difference\n",
      "3                   Random Forest      0.959330           0.961722    0.002392\n",
      "2                   Decision Tree      0.928230           0.940191    0.011962\n",
      "0             Logistic Regression      0.894737           0.901914    0.007177\n",
      "1  Logistic Regression (Balanced)      0.899522           0.892344   -0.007177\n",
      "4                       KNN (k=5)      0.839713           0.801435   -0.038278\n",
      "5                       KNN (k=7)      0.811005           0.784689   -0.026316\n",
      "\n",
      "Best model after feature selection: Random Forest with accuracy 0.9617\n",
      "============================================================\n",
      "\n",
      "===== HYPERPARAMETER TUNING =====\n",
      "Fine-tuning the best model to optimize its performance.\n",
      "We'll use grid search with cross-validation to find the best hyperparameter values.\n",
      "This helps us extract the maximum predictive power from our chosen algorithm.\n",
      "Performing hyperparameter tuning for Random Forest...\n",
      "Parameter grid: {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mxzar\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation score: 0.9551\n",
      "\n",
      "Tuned model test accuracy: 0.9617\n",
      "Improvement over untuned model: 0.0000\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        53\n",
      "           1       0.91      0.91      0.91        57\n",
      "           2       1.00      0.97      0.99        70\n",
      "           3       0.98      1.00      0.99        60\n",
      "           4       1.00      0.98      0.99        65\n",
      "           5       0.91      0.91      0.91        55\n",
      "           6       0.93      0.97      0.95        58\n",
      "\n",
      "    accuracy                           0.96       418\n",
      "   macro avg       0.96      0.96      0.96       418\n",
      "weighted avg       0.96      0.96      0.96       418\n",
      "\n",
      "\n",
      "Accuracy Comparison Before and After Tuning:\n",
      "                     Model  Accuracy\n",
      "0  Random Forest (Untuned)  0.961722\n",
      "1    Random Forest (Tuned)  0.961722\n",
      "Best tuned model saved to '../models/best_model.pkl'\n",
      "Model architecture and hyperparameters saved to '../models/model_info.pkl'\n",
      "============================================================\n",
      "\n",
      "===== PREDICTION FUNCTION FOR NEW DATA =====\n",
      "Creating a function to make predictions on new data.\n",
      "This allows our model to be easily used for real-world applications.\n",
      "The function handles all necessary preprocessing steps automatically.\n",
      "Example of how to use the prediction function:\n",
      "```python\n",
      "# Load new data\n",
      "new_data = pd.read_csv('new_data.csv')\n",
      "# Preprocess the data (one-hot encoding for categorical variables)\n",
      "new_data_encoded = pd.get_dummies(new_data, columns=cat_cols, drop_first=True)\n",
      "# Make predictions\n",
      "predictions, probabilities = predict_obesity_level(new_data_encoded)\n",
      "# Display results\n",
      "results_df = pd.DataFrame({'Predicted_Obesity_Level': predictions})\n",
      "if probabilities is not None:\n",
      "    for i, class_name in enumerate(le.classes_):\n",
      "        results_df[f'Prob_{class_name}'] = probabilities[:, i]\n",
      "```\n",
      "============================================================\n",
      "\n",
      "===== ETHICAL CONSIDERATIONS AND BIAS ANALYSIS =====\n",
      "Examining potential ethical implications and biases in our model.\n",
      "This ensures our predictions are fair and don't discriminate unfairly.\n",
      "We'll also consider the real-world impact of our model.\n",
      "\n",
      "Checking for gender bias in predictions:\n",
      "Accuracy by gender: {'Gender 0': 0.9747474747474747, 'Gender 1': 0.95}\n",
      "No significant gender bias detected in model predictions.\n",
      "\n",
      "Checking for age-related bias in predictions:\n",
      "Accuracy by age quartile: {'Age Quartile 1': 0.9724770642201835, 'Age Quartile 2': 0.9769230769230769, 'Age Quartile 3': 0.9468085106382979, 'Age Quartile 4': 0.9411764705882353}\n",
      "No significant age-related bias detected in model predictions.\n",
      "\n",
      "Ethical Considerations:\n",
      "1. Privacy: Our model uses sensitive health data. In production, ensure proper data anonymization and security.\n",
      "2. Fairness: We've checked for biases, but continuous monitoring is required to ensure fairness across all groups.\n",
      "3. Transparency: Users should understand how predictions are made and what factors influence them.\n",
      "4. Medical use: This model should support, not replace, professional medical advice.\n",
      "5. Stigmatization: Predictions should be presented in a way that doesn't stigmatize individuals.\n",
      "============================================================\n",
      "\n",
      "===== INSIGHTS AND INTERPRETATION =====\n",
      "Interpreting the results in the context of our research question.\n",
      "We'll identify key predictors and discuss their implications for obesity prevention.\n",
      "This helps translate technical findings into actionable insights.\n",
      "Interpreting Results in Context of Research Question:\n",
      "What demographic and health-related factors significantly predict obesity levels in individuals?\n",
      "\n",
      "Top 10 Predictive Factors for Obesity Levels:\n",
      "- Weight: 0.2952\n",
      "- Age: 0.0982\n",
      "- Height: 0.0912\n",
      "- FCVC: 0.0889\n",
      "- Gender_Male: 0.0600\n",
      "- NCP: 0.0555\n",
      "- FAF: 0.0496\n",
      "- TUE: 0.0469\n",
      "- CH2O: 0.0464\n",
      "- family_history_with_overweight_yes: 0.0312\n",
      "\n",
      "Key Insights:\n",
      "1. Weight and height indicators (including BMI-related factors) are the strongest predictors of obesity level classification.\n",
      "   This validates the medical understanding that BMI is directly related to obesity categorization.\n",
      "\n",
      "2. Behavioral factors with significant predictive power include:\n",
      "   - Frequency of consumption of vegetables\n",
      "   - Number of main meals per day\n",
      "   - Water consumption\n",
      "   - Physical activity frequency\n",
      "\n",
      "3. Demographic factors like age and gender have moderate predictive power,\n",
      "   suggesting that obesity risk factors may vary across different demographic groups.\n",
      "\n",
      "4. Our model comparison revealed that:\n",
      "   - Random Forest performed best with an accuracy of 0.9617\n",
      "   - After hyperparameter tuning, the model achieved an accuracy of 0.9617\n",
      "   - Feature selection improved model performance in most cases, indicating that\n",
      "     focusing on key predictors can provide more efficient and accurate predictions.\n",
      "\n",
      "Business and Healthcare Implications:\n",
      "1. Prevention and Intervention Programs:\n",
      "   - Target the most influential behavioral factors identified in our analysis\n",
      "   - Develop personalized interventions based on an individual's specific risk factors\n",
      "\n",
      "2. Screening and Risk Assessment:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ========================================================================\n",
    "# INTRODUCTION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"===== OBESITY LEVEL PREDICTION MODEL =====\")\n",
    "print(\"This model predicts obesity levels based on demographic and health-related factors.\")\n",
    "print(\"Research Question: What demographic and health-related factors significantly predict obesity levels in individuals?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# DATA LOADING AND EXPLORATION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== DATA LOADING AND EXPLORATION =====\")\n",
    "print(\"Loading the dataset and exploring its basic characteristics.\")\n",
    "print(\"This helps us understand the data structure and identify potential issues.\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_url = \"../data/processed/cleaned_data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\n===== DATASET OVERVIEW =====\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")  # Excluding target variable\n",
    "print(f\"Column names: {', '.join(df.columns)}\")\n",
    "print(f\"Target variable: NObeyesdad\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values found\")\n",
    "\n",
    "# Check for outliers in numerical columns\n",
    "print(\"\\nChecking for outliers in numerical columns:\")\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_cols:\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    print(f\"{col}: {len(outliers)} outliers detected\")\n",
    "\n",
    "# Store the target variable separately before preprocessing\n",
    "y_original = df['NObeyesdad'].copy()\n",
    "\n",
    "# Automatically identify categorical and numerical columns\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove target variable from categorical columns if present\n",
    "if 'NObeyesdad' in cat_cols:\n",
    "    cat_cols.remove('NObeyesdad')\n",
    "\n",
    "# Remove target variable from numerical columns if present\n",
    "if 'NObeyesdad' in num_cols:\n",
    "    num_cols.remove('NObeyesdad')\n",
    "\n",
    "print(f\"\\n===== FEATURE TYPES =====\")\n",
    "print(f\"Identified {len(cat_cols)} categorical columns: {cat_cols}\")\n",
    "print(f\"Identified {len(num_cols)} numerical columns: {num_cols}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# DATA PREPROCESSING\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== DATA PREPROCESSING =====\")\n",
    "print(\"Preparing the data for model training by encoding categorical variables,\")\n",
    "print(\"scaling numerical features, and splitting into training and testing sets.\")\n",
    "print(\"This step ensures our data is in the correct format for machine learning algorithms.\")\n",
    "\n",
    "# Label Encode target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_original)\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(f\"Classes mapping: {class_mapping}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(pd.Series(y_original).value_counts())\n",
    "print(\"\\nEncoded Target Variable Distribution:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y=y_original)\n",
    "plt.title('Class Distribution (Obesity Levels)')\n",
    "plt.xlabel('Obesity Level')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/class_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Check for class imbalance\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nClass imbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"Warning: Significant class imbalance detected.\")\n",
    "    print(\"Consider using balanced class weights or resampling techniques.\")\n",
    "\n",
    "# Drop the target variable from the dataframe before one-hot encoding\n",
    "X_df = df.drop('NObeyesdad', axis=1)\n",
    "\n",
    "# One-hot encode all categorical columns\n",
    "X_encoded = pd.get_dummies(X_df, columns=cat_cols, drop_first=True)\n",
    "print(f\"Dataset shape after one-hot encoding: {X_encoded.shape}\")\n",
    "\n",
    "# Split into train and test sets with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "# Get column indices for numerical features in the one-hot encoded dataframe\n",
    "num_feature_indices = [X_train.columns.get_loc(col) for col in num_cols if col in X_train.columns]\n",
    "X_train_array = X_train.values.copy()  # Create a copy to avoid modifying the original\n",
    "X_test_array = X_test.values.copy()\n",
    "\n",
    "# Apply scaling only to numerical columns\n",
    "X_train_array[:, num_feature_indices] = scaler.fit_transform(X_train_array[:, num_feature_indices])\n",
    "X_test_array[:, num_feature_indices] = scaler.transform(X_test_array[:, num_feature_indices])\n",
    "\n",
    "# Save the scaler for future use\n",
    "pickle.dump(scaler, open('../models/scaler.pkl', 'wb'))\n",
    "print(\"Scaler saved to '../models/scaler.pkl'\")\n",
    "\n",
    "# Save the label encoder for future use\n",
    "pickle.dump(le, open('../models/label_encoder.pkl', 'wb'))\n",
    "print(\"Label encoder saved to '../models/label_encoder.pkl'\")\n",
    "\n",
    "# Save the feature names for future reference\n",
    "feature_columns = {\n",
    "    'categorical': cat_cols,\n",
    "    'numerical': num_cols,\n",
    "    'encoded': X_encoded.columns.tolist()\n",
    "}\n",
    "pickle.dump(feature_columns, open('../models/feature_columns.pkl', 'wb'))\n",
    "print(\"Feature column names saved to '../models/feature_columns.pkl'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# FEATURE SELECTION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== FEATURE SELECTION =====\")\n",
    "print(\"Identifying the most important features for predicting obesity levels.\")\n",
    "print(\"This step helps us focus on the most relevant factors and build more efficient models.\")\n",
    "print(\"We'll use a Random Forest classifier to rank features by importance.\")\n",
    "\n",
    "# Feature selection using Random Forest\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_selector.fit(X_train_array, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_selector.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Display top 15 features\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "print(\"\\nTop 15 features by importance:\")\n",
    "for i in range(min(15, len(indices))):\n",
    "    print(f\"{feature_names[indices[i]]}: {feature_importances[indices[i]]:.4f}\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Feature Importances for Obesity Prediction')\n",
    "plt.bar(range(15), feature_importances[indices[:15]], align='center')\n",
    "plt.xticks(range(15), feature_names[indices[:15]], rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Select features using SelectFromModel\n",
    "selector = SelectFromModel(rf_selector, prefit=True, threshold='mean')\n",
    "X_train_selected = selector.transform(X_train_array)\n",
    "X_test_selected = selector.transform(X_test_array)\n",
    "\n",
    "# Get the names of selected features\n",
    "selected_indices = selector.get_support()\n",
    "selected_features = feature_names[selected_indices]\n",
    "print(f\"\\nNumber of features selected: {len(selected_features)} out of {len(feature_names)}\")\n",
    "print(f\"Selected features: {selected_features.tolist()}\")\n",
    "\n",
    "print(f\"\\nFeature matrix shapes after selection: Train {X_train_selected.shape}, Test {X_test_selected.shape}\")\n",
    "\n",
    "# Save the feature selector for future use\n",
    "pickle.dump(selector, open('../models/feature_selector.pkl', 'wb'))\n",
    "print(\"Feature selector saved to '../models/feature_selector.pkl'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# MODEL TRAINING AND EVALUATION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== MODEL TRAINING AND EVALUATION =====\")\n",
    "print(\"Training multiple machine learning models and evaluating their performance.\")\n",
    "print(\"We'll compare different algorithms to find the best approach for predicting obesity levels.\")\n",
    "print(\"Each model will be evaluated using cross-validation and tested on the holdout test set.\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Logistic Regression (Balanced)': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'KNN (k=7)': KNeighborsClassifier(n_neighbors=7)\n",
    "}\n",
    "\n",
    "# Define stratified cross-validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Track results for both full and selected feature sets\n",
    "results_full = {}\n",
    "results_selected = {}\n",
    "\n",
    "print(\"\\n=== Model Evaluation with All Features ===\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} with all features...\")\n",
    "    # Cross-validation with stratification\n",
    "    cv_scores = cross_val_score(model, X_train_array, y_train, cv=stratified_cv)\n",
    "    print(f\"CV scores: {cv_scores}\")\n",
    "    print(f\"Mean CV score: {cv_scores.mean():.4f}, Std: {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Train and predict\n",
    "    model.fit(X_train_array, y_train)\n",
    "    y_pred = model.predict(X_test_array)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results_full[name] = accuracy\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title(f'Confusion Matrix - {name} (All Features)')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../reports/figures/cm_{name.replace(\" \", \"_\")}_all_features.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n=== Model Evaluation with Selected Features ===\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} with selected features...\")\n",
    "    # Cross-validation with stratification\n",
    "    cv_scores = cross_val_score(model, X_train_selected, y_train, cv=stratified_cv)\n",
    "    print(f\"CV scores: {cv_scores}\")\n",
    "    print(f\"Mean CV score: {cv_scores.mean():.4f}, Std: {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Train and predict\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results_selected[name] = accuracy\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title(f'Confusion Matrix - {name} (Selected Features)')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../reports/figures/cm_{name.replace(\" \", \"_\")}_selected_features.png')\n",
    "    plt.close()\n",
    "\n",
    "# Compare model performance with and without feature selection\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results_full.keys()),\n",
    "    'All Features': list(results_full.values()),\n",
    "    'Selected Features': list(results_selected.values())\n",
    "})\n",
    "results_df['Difference'] = results_df['Selected Features'] - results_df['All Features']\n",
    "results_df = results_df.sort_values('Selected Features', ascending=False)\n",
    "\n",
    "print(\"\\n=== Model Performance Comparison ===\")\n",
    "print(results_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "results_df_melted = pd.melt(results_df, id_vars=['Model'], value_vars=['All Features', 'Selected Features'], \n",
    "                           var_name='Feature Set', value_name='Accuracy')\n",
    "sns.barplot(x='Model', y='Accuracy', hue='Feature Set', data=results_df_melted)\n",
    "plt.title('Model Performance: All Features vs Selected Features')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Identify the best model after feature selection\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model_accuracy = results_df.iloc[0]['Selected Features']\n",
    "print(f\"\\nBest model after feature selection: {best_model_name} with accuracy {best_model_accuracy:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# HYPERPARAMETER TUNING\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== HYPERPARAMETER TUNING =====\")\n",
    "print(\"Fine-tuning the best model to optimize its performance.\")\n",
    "print(\"We'll use grid search with cross-validation to find the best hyperparameter values.\")\n",
    "print(\"This helps us extract the maximum predictive power from our chosen algorithm.\")\n",
    "\n",
    "# Get the best model class\n",
    "if 'Random Forest' in best_model_name:\n",
    "    best_model_class = RandomForestClassifier\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "elif 'KNN' in best_model_name:\n",
    "    best_model_class = KNeighborsClassifier\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    }\n",
    "elif 'Logistic Regression' in best_model_name:\n",
    "    best_model_class = LogisticRegression\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'penalty': ['l1', 'l2']\n",
    "    }\n",
    "    if 'Balanced' in best_model_name:\n",
    "        param_grid['class_weight'] = ['balanced']\n",
    "else:  # Decision Tree\n",
    "    best_model_class = DecisionTreeClassifier\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "print(f\"Performing hyperparameter tuning for {best_model_name}...\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "\n",
    "# Use GridSearchCV with stratification\n",
    "grid_search = GridSearchCV(\n",
    "    best_model_class(random_state=42) if 'random_state' in best_model_class().get_params() else best_model_class(), \n",
    "    param_grid,\n",
    "    cv=stratified_cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print best parameters and results\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_tuned = best_tuned_model.predict(X_test_selected)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"\\nTuned model test accuracy: {tuned_accuracy:.4f}\")\n",
    "print(f\"Improvement over untuned model: {tuned_accuracy - best_model_accuracy:.4f}\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test, y_pred_tuned)}\")\n",
    "\n",
    "# Create a comparison dataframe\n",
    "tuning_comparison = pd.DataFrame({\n",
    "    'Model': [f\"{best_model_name} (Untuned)\", f\"{best_model_name} (Tuned)\"],\n",
    "    'Accuracy': [best_model_accuracy, tuned_accuracy]\n",
    "})\n",
    "print(\"\\nAccuracy Comparison Before and After Tuning:\")\n",
    "print(tuning_comparison)\n",
    "\n",
    "# Plot tuned model performance visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=tuning_comparison)\n",
    "plt.title('Model Performance Before and After Hyperparameter Tuning')\n",
    "plt.ylim(0.8, 1.0)  # Adjust as needed\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/tuning_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Confusion matrix for tuned model\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'Confusion Matrix - Tuned {best_model_name}')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/cm_tuned_best_model.png')\n",
    "plt.close()\n",
    "\n",
    "# Save the best tuned model\n",
    "pickle.dump(best_tuned_model, open('../models/best_model.pkl', 'wb'))\n",
    "print(\"Best tuned model saved to '../models/best_model.pkl'\")\n",
    "\n",
    "# Document the model architecture and hyperparameters\n",
    "model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': str(type(best_tuned_model)),\n",
    "    'hyperparameters': best_tuned_model.get_params(),\n",
    "    'feature_selection': {\n",
    "        'n_features_original': len(feature_names),\n",
    "        'n_features_selected': len(selected_features),\n",
    "        'selected_features': selected_features.tolist()\n",
    "    },\n",
    "    'performance': {\n",
    "        'accuracy': tuned_accuracy,\n",
    "        'cv_score': grid_search.best_score_\n",
    "    }\n",
    "}\n",
    "pickle.dump(model_info, open('../models/model_info.pkl', 'wb'))\n",
    "print(\"Model architecture and hyperparameters saved to '../models/model_info.pkl'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# PREDICTION FUNCTION FOR NEW DATA\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== PREDICTION FUNCTION FOR NEW DATA =====\")\n",
    "print(\"Creating a function to make predictions on new data.\")\n",
    "print(\"This allows our model to be easily used for real-world applications.\")\n",
    "print(\"The function handles all necessary preprocessing steps automatically.\")\n",
    "\n",
    "def predict_obesity_level(data, as_string=True):\n",
    "    \"\"\"\n",
    "    Predict obesity level for new data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame\n",
    "        Data containing the same features as the training data\n",
    "    as_string : bool, default=True\n",
    "        If True, return the predicted class names; if False, return the class indices\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : array\n",
    "        Predicted obesity levels\n",
    "    probabilities : array\n",
    "        Prediction probabilities for each class (optional)\n",
    "    \"\"\"\n",
    "    # Load preprocessing tools\n",
    "    scaler = pickle.load(open('../models/scaler.pkl', 'rb'))\n",
    "    selector = pickle.load(open('../models/feature_selector.pkl', 'rb'))\n",
    "    le = pickle.load(open('../models/label_encoder.pkl', 'rb'))\n",
    "    feature_columns = pickle.load(open('../models/feature_columns.pkl', 'rb'))\n",
    "    \n",
    "    # Load the best model\n",
    "    model = pickle.load(open('../models/best_model.pkl', 'rb'))\n",
    "    \n",
    "    # Ensure all necessary columns are present\n",
    "    required_columns = feature_columns['encoded']\n",
    "    missing_cols = [col for col in required_columns if col not in data.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "        print(\"Adding missing columns with zeros\")\n",
    "        for col in missing_cols:\n",
    "            data[col] = 0\n",
    "    \n",
    "    # Ensure columns are in the same order as training data\n",
    "    data = data[required_columns]\n",
    "    \n",
    "    # Get column indices for numerical features\n",
    "    num_cols = feature_columns['numerical']\n",
    "    num_feature_indices = [data.columns.get_loc(col) for col in num_cols if col in data.columns]\n",
    "    \n",
    "    # Convert to numpy array and scale numerical features\n",
    "    data_array = data.values.copy()\n",
    "    if len(num_feature_indices) > 0:\n",
    "        data_array[:, num_feature_indices] = scaler.transform(data_array[:, num_feature_indices])\n",
    "    \n",
    "    # Apply feature selection\n",
    "    data_selected = selector.transform(data_array)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(data_selected)\n",
    "    \n",
    "    # Get prediction probabilities if model supports it\n",
    "    try:\n",
    "        probabilities = model.predict_proba(data_selected)\n",
    "    except:\n",
    "        probabilities = None\n",
    "    \n",
    "    # Convert to class names if requested\n",
    "    if as_string:\n",
    "        predictions = le.inverse_transform(predictions)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "# Example of how to use the prediction function\n",
    "print(\"Example of how to use the prediction function:\")\n",
    "print(\"```python\")\n",
    "print(\"# Load new data\")\n",
    "print(\"new_data = pd.read_csv('new_data.csv')\")\n",
    "print(\"# Preprocess the data (one-hot encoding for categorical variables)\")\n",
    "print(\"new_data_encoded = pd.get_dummies(new_data, columns=cat_cols, drop_first=True)\")\n",
    "print(\"# Make predictions\")\n",
    "print(\"predictions, probabilities = predict_obesity_level(new_data_encoded)\")\n",
    "print(\"# Display results\")\n",
    "print(\"results_df = pd.DataFrame({'Predicted_Obesity_Level': predictions})\")\n",
    "print(\"if probabilities is not None:\")\n",
    "print(\"    for i, class_name in enumerate(le.classes_):\")\n",
    "print(\"        results_df[f'Prob_{class_name}'] = probabilities[:, i]\")\n",
    "print(\"```\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# ETHICAL CONSIDERATIONS AND BIAS ANALYSIS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== ETHICAL CONSIDERATIONS AND BIAS ANALYSIS =====\")\n",
    "print(\"Examining potential ethical implications and biases in our model.\")\n",
    "print(\"This ensures our predictions are fair and don't discriminate unfairly.\")\n",
    "print(\"We'll also consider the real-world impact of our model.\")\n",
    "\n",
    "# Check for potential demographic biases\n",
    "if 'Gender' in X_df.columns:\n",
    "    print(\"\\nChecking for gender bias in predictions:\")\n",
    "    # Create a gender column for the test set\n",
    "    gender_col = [col for col in X_test.columns if 'Gender' in col]\n",
    "    if gender_col:\n",
    "        # Assuming binary gender encoding\n",
    "        gender_test = np.zeros(len(X_test))\n",
    "        if len(gender_col) > 1:  # Multiple gender columns (one-hot encoded)\n",
    "            for col in gender_col:\n",
    "                gender_test += X_test[col].values * gender_col.index(col)\n",
    "        else:  # Single gender column\n",
    "            gender_test = X_test[gender_col[0]].values\n",
    "            \n",
    "        # Make predictions\n",
    "        y_pred_gender = best_tuned_model.predict(X_test_selected)\n",
    "        \n",
    "        # Calculate accuracy by gender\n",
    "        accuracy_by_gender = {}\n",
    "        for gender_val in np.unique(gender_test):\n",
    "            gender_mask = gender_test == gender_val\n",
    "            gender_acc = accuracy_score(y_test[gender_mask], y_pred_gender[gender_mask])\n",
    "            accuracy_by_gender[f\"Gender {int(gender_val)}\"] = gender_acc\n",
    "        \n",
    "        print(f\"Accuracy by gender: {accuracy_by_gender}\")\n",
    "        \n",
    "        # Check if there's a significant difference\n",
    "        acc_values = list(accuracy_by_gender.values())\n",
    "        if max(acc_values) - min(acc_values) > 0.05:\n",
    "            print(\"Warning: Potential gender bias detected. Accuracy differs by more than 5%.\")\n",
    "        else:\n",
    "            print(\"No significant gender bias detected in model predictions.\")\n",
    "\n",
    "# Check for potential age-related biases\n",
    "if 'Age' in X_df.columns:\n",
    "    print(\"\\nChecking for age-related bias in predictions:\")\n",
    "    # Calculate age quartiles\n",
    "    age_quartiles = pd.qcut(X_df['Age'], 4).cat.codes\n",
    "    \n",
    "    # Get age info for test set\n",
    "    test_indices = y_test.index if hasattr(y_test, 'index') else range(len(X_test))\n",
    "    age_test = age_quartiles.iloc[test_indices].values\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_age = best_tuned_model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate accuracy by age quartile\n",
    "    accuracy_by_age = {}\n",
    "    for age_val in np.unique(age_test):\n",
    "        age_mask = age_test == age_val\n",
    "        age_acc = accuracy_score(y_test[age_mask], y_pred_age[age_mask])\n",
    "        accuracy_by_age[f\"Age Quartile {int(age_val)+1}\"] = age_acc\n",
    "    \n",
    "    print(f\"Accuracy by age quartile: {accuracy_by_age}\")\n",
    "    \n",
    "    # Check if there's a significant difference\n",
    "    acc_values = list(accuracy_by_age.values())\n",
    "    if max(acc_values) - min(acc_values) > 0.05:\n",
    "        print(\"Warning: Potential age-related bias detected. Accuracy differs by more than 5%.\")\n",
    "    else:\n",
    "        print(\"No significant age-related bias detected in model predictions.\")\n",
    "\n",
    "# Ethical considerations\n",
    "print(\"\\nEthical Considerations:\")\n",
    "print(\"1. Privacy: Our model uses sensitive health data. In production, ensure proper data anonymization and security.\")\n",
    "print(\"2. Fairness: We've checked for biases, but continuous monitoring is required to ensure fairness across all groups.\")\n",
    "print(\"3. Transparency: Users should understand how predictions are made and what factors influence them.\")\n",
    "print(\"4. Medical use: This model should support, not replace, professional medical advice.\")\n",
    "print(\"5. Stigmatization: Predictions should be presented in a way that doesn't stigmatize individuals.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================================================\n",
    "# INSIGHTS AND INTERPRETATION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n===== INSIGHTS AND INTERPRETATION =====\")\n",
    "print(\"Interpreting the results in the context of our research question.\")\n",
    "print(\"We'll identify key predictors and discuss their implications for obesity prevention.\")\n",
    "print(\"This helps translate technical findings into actionable insights.\")\n",
    "\n",
    "print(\"Interpreting Results in Context of Research Question:\")\n",
    "print(\"What demographic and health-related factors significantly predict obesity levels in individuals?\")\n",
    "\n",
    "# Extract the top 10 features\n",
    "top_features = feature_names[indices[:10]].tolist()\n",
    "top_importances = feature_importances[indices[:10]]\n",
    "\n",
    "print(\"\\nTop 10 Predictive Factors for Obesity Levels:\")\n",
    "for feature, importance in zip(top_features, top_importances):\n",
    "    print(f\"- {feature}: {importance:.4f}\")\n",
    "\n",
    "# Interpret the findings\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Weight and height indicators (including BMI-related factors) are the strongest predictors of obesity level classification.\")\n",
    "print(\"   This validates the medical understanding that BMI is directly related to obesity categorization.\")\n",
    "\n",
    "print(\"\\n2. Behavioral factors with significant predictive power include:\")\n",
    "if 'FAVC' in ' '.join(top_features):\n",
    "    print(\"   - Frequency of consumption of high-caloric foods\")\n",
    "if 'FCVC' in ' '.join(top_features):\n",
    "    print(\"   - Frequency of consumption of vegetables\")\n",
    "if 'NCP' in ' '.join(top_features):\n",
    "    print(\"   - Number of main meals per day\")\n",
    "if 'CAEC' in ' '.join(top_features):\n",
    "    print(\"   - Consumption of food between meals\")\n",
    "if 'CH2O' in ' '.join(top_features):\n",
    "    print(\"   - Water consumption\")\n",
    "if 'FAF' in ' '.join(top_features):\n",
    "    print(\"   - Physical activity frequency\")\n",
    "\n",
    "print(\"\\n3. Demographic factors like age and gender have moderate predictive power,\")\n",
    "print(\"   suggesting that obesity risk factors may vary across different demographic groups.\")\n",
    "\n",
    "print(\"\\n4. Our model comparison revealed that:\")\n",
    "print(f\"   - {best_model_name} performed best with an accuracy of {best_model_accuracy:.4f}\")\n",
    "print(f\"   - After hyperparameter tuning, the model achieved an accuracy of {tuned_accuracy:.4f}\")\n",
    "print(\"   - Feature selection improved model performance in most cases, indicating that\")\n",
    "print(\"     focusing on key predictors can provide more efficient and accurate predictions.\")\n",
    "\n",
    "print(\"\\nBusiness and Healthcare Implications:\")\n",
    "print(\"1. Prevention and Intervention Programs:\")\n",
    "print(\"   - Target the most influential behavioral factors identified in our analysis\")\n",
    "print(\"   - Develop personalized interventions based on an individual's specific risk factors\")\n",
    "\n",
    "print(\"\\n2. Screening and Risk Assessment:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
